\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\newlabel{FirstPage}{{}{1}{}{section*.1}{}}
\@writefile{toc}{\contentsline {title}{FYS-STK3155 - Project 2}{1}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {abstract}{Abstract}{1}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {II}Method}{1}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Stochastic gradient descent}{1}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Neural networks}{1}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Accuracy score}{1}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D}Regression methods}{1}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {E}Datasets}{2}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}The Franke function}{2}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}MNIST digits dataset}{2}{section*.11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Examples of 8x8 pixels pictures of handwritten figures of the MNIST dataset.}}{2}{figure.1}\protected@file@percent }
\newlabel{figure}{{1}{2}{Examples of 8x8 pixels pictures of handwritten figures of the MNIST dataset}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Results}{2}{section*.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Mean squared error score for Linear regression using SGD }}{2}{figure.2}\protected@file@percent }
\newlabel{figure}{{2}{2}{Mean squared error score for Linear regression using SGD}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Mean squared error score for Ridge regression using SGD }}{2}{figure.3}\protected@file@percent }
\newlabel{figure}{{3}{2}{Mean squared error score for Ridge regression using SGD}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Mean squared error score for SGD Regression using Sklearn with regularization parameter lambda, $\lambda $. }}{2}{figure.4}\protected@file@percent }
\newlabel{figure}{{4}{2}{Mean squared error score for SGD Regression using Sklearn with regularization parameter lambda, $\lambda $}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Mean squared error score for 3 models, OLS, a neural net class and a Scikit-learn neural net over selected model complexities (Polynomial degree) and different learning rates. }}{3}{figure.5}\protected@file@percent }
\newlabel{figure}{{5}{3}{Mean squared error score for 3 models, OLS, a neural net class and a Scikit-learn neural net over selected model complexities (Polynomial degree) and different learning rates}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The Mean of the MSE score for all polynomial degrees for the NN class and the Scikit-learn NN, ploted against the range of learning rates.}}{3}{figure.6}\protected@file@percent }
\newlabel{figure}{{6}{3}{The Mean of the MSE score for all polynomial degrees for the NN class and the Scikit-learn NN, ploted against the range of learning rates}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Mean squared error scores for Ridge regression over values of Lambda and Polynomial degrees. }}{3}{figure.7}\protected@file@percent }
\newlabel{figure}{{7}{3}{Mean squared error scores for Ridge regression over values of Lambda and Polynomial degrees}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Mean squared error scores for Neural network regression over values of Lambda and Polynomial degrees.}}{3}{figure.8}\protected@file@percent }
\newlabel{figure}{{8}{3}{Mean squared error scores for Neural network regression over values of Lambda and Polynomial degrees}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Mean squared error scores for the Scikit-learn Neural network regression over values of Lambda and Polynomial degrees}}{3}{figure.9}\protected@file@percent }
\newlabel{figure}{{9}{3}{Mean squared error scores for the Scikit-learn Neural network regression over values of Lambda and Polynomial degrees}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Mean squared error scores for all four models over Polynomial degree, plottet at selected values of lambda. }}{4}{figure.10}\protected@file@percent }
\newlabel{figure}{{10}{4}{Mean squared error scores for all four models over Polynomial degree, plottet at selected values of lambda}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Log(MSE) for neural network implementation using RELU and leaky RELU activation functions for the hidden layer over polynomial degrees. }}{4}{figure.11}\protected@file@percent }
\newlabel{figure}{{11}{4}{Log(MSE) for neural network implementation using RELU and leaky RELU activation functions for the hidden layer over polynomial degrees}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces MSE score for RELU and leaky RELU implementation of my neural network at $lambda=0$. RELU activation for the Scikit-learn implementation. }}{4}{figure.12}\protected@file@percent }
\newlabel{figure}{{12}{4}{MSE score for RELU and leaky RELU implementation of my neural network at $lambda=0$. RELU activation for the Scikit-learn implementation}{figure.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Classification results}{5}{section*.13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Accuracy score over a range of 20 learning rates from log(-5) to log(-1)}}{5}{figure.13}\protected@file@percent }
\newlabel{figure}{{13}{5}{Accuracy score over a range of 20 learning rates from log(-5) to log(-1)}{figure.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Accuracy score for the neural network over learning rates and regularization parameter lambda.}}{5}{figure.14}\protected@file@percent }
\newlabel{figure}{{14}{5}{Accuracy score for the neural network over learning rates and regularization parameter lambda}{figure.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Accuracy score for the Scikit-learn neural network over learning rates and regularization parameter lambda. }}{5}{figure.15}\protected@file@percent }
\newlabel{figure}{{15}{5}{Accuracy score for the Scikit-learn neural network over learning rates and regularization parameter lambda}{figure.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Accuracy score for the neural network with two hidden layers with 100 nodes each over learning rates and regularization parameter lambda. }}{5}{figure.16}\protected@file@percent }
\newlabel{figure}{{16}{5}{Accuracy score for the neural network with two hidden layers with 100 nodes each over learning rates and regularization parameter lambda}{figure.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Accuracy score of logistic regression on the MNIST dataset plotted against a range of regularization parameters.}}{5}{figure.17}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {V}Discussion}{5}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Classification}{6}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{6}{section*.16}\protected@file@percent }
\bibdata{project2Notes}
\bibstyle{apsrev4-2}
\citation{REVTEX42Control}
\citation{apsrev42Control}
\@writefile{toc}{\contentsline {section}{\numberline {}References}{7}{section*.17}\protected@file@percent }
\newlabel{LastPage}{{}{7}{}{}{}}
